1
00:00:00,000 --> 00:00:03,000
Can AI help us learn?

2
00:00:03,000 --> 00:00:10,000
Some of you might be thinking, of course, it's so powerful, it can do so many things, customize them for us.

3
00:00:10,000 --> 00:00:21,000
But I want to say that the biggest revolution AI is bringing to education is not that it's going to make math more fun for you or it's going to explain Shakespeare like your five years old.

4
00:00:21,000 --> 00:00:28,000
The biggest revolution AI is bringing to education is that it's highlighting the systems failed incentives.

5
00:00:28,000 --> 00:00:37,000
Because why should anybody study when we've told them the whole time that all that matters at the end isn't the process, it's the A+.

6
00:00:37,000 --> 00:00:46,000
Why should they actually put in all the hard work to write draft after draft for an essay when the feedback is just B?

7
00:00:46,000 --> 00:00:51,000
No extra notes, nothing to motivate them to want to learn harder.

8
00:00:51,000 --> 00:01:08,000
And these companies are much faster than our institutions. Most recently, open AI, Google and Thropic have all been giving away their most powerful models for free until the end of May, which as you might guess is exactly during the time of finals.

9
00:01:08,000 --> 00:01:18,000
So we are putting these tools completely unregulated in front of vulnerable students right at the time where they're most desperate to use them.

10
00:01:18,000 --> 00:01:25,000
And yet these companies will say AI is going to revolutionize education, particularly through personalization.

11
00:01:25,000 --> 00:01:34,000
Company after company will say that through personalized tutoring AI is going to revolutionize education and make it so much better for everyone.

12
00:01:34,000 --> 00:01:39,000
And why not, right? The image of a one-on-one tutor is so compelling.

13
00:01:39,000 --> 00:01:46,000
Talking to a person, feeling this connection, getting my education customized just for me, it sounds amazing.

14
00:01:47,000 --> 00:02:04,000
And today, education looks like this. A teacher in front of an army of students, and if the technologists believe that the ideal looks like this, one teacher, one student, then why not just flip out the teacher for an AI?

15
00:02:04,000 --> 00:02:12,000
And then the next step is an army of AI's with an army of students. It sounds like a perfect model of education.

16
00:02:12,000 --> 00:02:24,000
Except perfection is now what we should strive for when it comes to learning. We don't want engineers that studied how to build bridges in perfect conditions, because the real world is everything but perfect. It's full of messes.

17
00:02:24,000 --> 00:02:33,000
And I missed all this noise. I don't see any of these companies asking, what is this student meant to learn with AI?

18
00:02:33,000 --> 00:02:38,000
Because if the idea is to make getting that A-plus easier, then I'm not interested.

19
00:02:38,000 --> 00:02:49,000
We're going to just waste 12 to 15 years of our lives, but more efficiently now. Doing exams, we're not going to remember a day after graduation.

20
00:02:49,000 --> 00:02:56,000
Now, for those of you out there who are educators like me, you might have noticed a discrepancy between my opening question and the follow-up statement.

21
00:02:56,000 --> 00:03:05,000
I asked, can AI help us learn? And I followed it up with the biggest revolution AI is bringing to education.

22
00:03:05,000 --> 00:03:16,000
But you might be feeling something, which is education is not learning. Education is a construct, something we as a society put our kids through.

23
00:03:16,000 --> 00:03:25,000
It's a system. But learning is a skill, a very human skill. And when we do it correctly, magical things can happen.

24
00:03:25,000 --> 00:03:34,000
We can motivate people to become their best selves. We can motivate people to work together and to contribute to society in the ways that we need to most.

25
00:03:34,000 --> 00:03:44,000
Now, in one of my classes, I like to sit around with students and work in the best way possible. I'm a university instructor, but it was really hard to find a bunch of 20-year-olds working nicely together.

26
00:03:44,000 --> 00:03:52,000
So I took this talk photo of a bunch of kids. And I noticed that one of the students had the price of her business model,

27
00:03:52,000 --> 00:04:07,000
which is how you're building small businesses and selling the products. She priced her business at $50 per month. And so I asked her, why did you price it that? And her answer, that's what GPD said.

28
00:04:07,000 --> 00:04:18,000
Now, some of you might say, obviously, it's not ideal, but isn't it very similar to what they were doing before? Kids were just saying, that's what I saw on Google.

29
00:04:18,000 --> 00:04:32,000
But I say it depends. It depends because on Google, when used correctly, we have all these sources to go through, multiple perspectives that we can look at and compare.

30
00:04:32,000 --> 00:04:41,000
But what happens is the magical allure of that first result. Everyone only clicks on that without looking at anything else.

31
00:04:41,000 --> 00:04:56,000
In this particular question, I asked, how can I price my business? And that first resulted from the BDC, an extremely reputable source, but is telling me how to price the business itself, not the services of my business. It didn't understand the query.

32
00:04:56,000 --> 00:05:06,000
Now, on chat GPT, if you type in, how should I price my business? It actually understands the query better. I don't know if you can read that up there, but it basically says there are multiple ways to price your services.

33
00:05:06,000 --> 00:05:24,000
So, amongst that is a lot of baseless advice with no sources. And that's a problem. Because even though chat GPT has a function where if you highlight something in the text, quotation marks appear, some of you might not know this, then you can click on those quotation marks and query that specific thing.

34
00:05:24,000 --> 00:05:33,000
But if people weren't clicking on the second result on Google, they're not going to use the power user features in chat GPT or any of these other AIs.

35
00:05:33,000 --> 00:05:40,000
And it's likely going to happen is you're going to scroll to the bottom of the query and they're going to type, OK, I get it.

36
00:05:40,000 --> 00:05:52,000
My business is like TurboTax. I help accountants calculate people's taxes. Tell me what number I should put there. And of course, nobody's reading the chat GPT might make mistakes subtitles, right?

37
00:05:52,000 --> 00:06:17,000
So, then chat GPT is going to spit out an essay, personalized to the language of the person, extremely compelling. And despite all the information on there, people are only going to look at that center piece, which if I zoom in, is the actual random answer of how much this person should charge for their business.

38
00:06:17,000 --> 00:06:27,000
The prompt, and this is a real prompt, had nothing but what I had on the screen. A small description of what the business does. No context for who the user is, nothing.

39
00:06:27,000 --> 00:06:43,000
And so, the student is participating in what's called cognitive offloading. They're effectively relinquishing their cognitive powers to a machine. And you can do this with people too. We do it with Google when we click on the first result without looking at anything else.

40
00:06:43,000 --> 00:07:02,000
The problem is how this is being understood. In NYU, a professor changed their assessment so that it could be harder for kids to use chat GPT. And the student replied that he is interfering with the student's learning style.

41
00:07:02,000 --> 00:07:17,000
Now, when you combine that with something that we see in technology, so I'm a UX designer in addition to being a university professor. And our job as user experience designers is to simplify the way people use technology.

42
00:07:17,000 --> 00:07:29,000
But there's a byproduct from that that arises, which is called a dark pattern. And what that means is, we can simplify a UX to the point that we might manipulate what the user's intention is.

43
00:07:29,000 --> 00:07:39,000
And I'll show you an example. Take the zoo. As you're buying the tickets and checking out, they want a donation. We all love a zoo. It's a charitable organization.

44
00:07:39,000 --> 00:07:51,000
Now, the arrow pointing to the right, and because we're English speakers or French speakers, we write right, you know, from left to right. The arrow pointing to the right is most likely what we're going to click on. It's dark, green, it's rich.

45
00:07:51,000 --> 00:08:01,000
But if you don't want to donate, you have to click on the one that looks like it's going to the back with the teeny tiny no donation over there. That is a user experience dark pattern.

46
00:08:01,000 --> 00:08:13,000
And when a chat GPT or large language model like it speaks to you in a perfect tone suited just to keep you on the on the tool, that is something very similar.

47
00:08:13,000 --> 00:08:25,000
According to this author, when a large language model constantly validates you and praises you causing you to spend more time on it, that's the same kind of thing as a dark pattern.

48
00:08:25,000 --> 00:08:42,000
And we see this already. A recent update of chat GPT that was rolled back, fortunately, praised a user for believing a conspiracy theory that led him to stop taking all his medications when he had heart palpitations and told him he is a brave individual for taking control of his own life.

49
00:08:42,000 --> 00:08:50,000
For isolation and for stopping his meds. And it doesn't just stop at students.

50
00:08:50,000 --> 00:09:03,000
Professionals have been tested and they are at risk as well. A researcher ran a study on over 300 professionals working in a large corporation, a tech company like Google Microsoft.

51
00:09:03,000 --> 00:09:08,000
And found that when they were tested on a variety of things, the results were quite stunning.

52
00:09:09,000 --> 00:09:17,000
Before I show you the chart, let's look at the key here. The two shades of blue are for much less effort and less effort respectively from dark blue to light blue.

53
00:09:17,000 --> 00:09:33,000
Now, when tested, the 390 and workers responded to a survey that when they used chat GPT on knowledge, they felt up to 70% of them responded that they feel they use less effort in their cognition.

54
00:09:33,000 --> 00:09:38,000
And that comes to comprehension of what they are reading, same thing.

55
00:09:38,000 --> 00:09:50,000
Assessment of the knowledge, synthesis, analysis and evaluation, all of these, at least 60% of people said that they felt they were putting in less effort.

56
00:09:50,000 --> 00:09:54,000
And that's extremely potent because these AIs are only going to get better.

57
00:09:54,000 --> 00:10:01,000
The same author of this study wrote a beautiful paper called, well, co-pilot becomes autopilot.

58
00:10:01,000 --> 00:10:12,000
And he says, the risk of moving to autopilot is an even greater challenge than the more commonly discussed issue of AI hallucinations or factual errors.

59
00:10:12,000 --> 00:10:23,000
Because the more pernicious outcome is that a generative AI becomes complicit in intellectual descaling and the atrophy of human critical thinking faculties.

60
00:10:25,000 --> 00:10:30,000
So, today, when you ask chat GPT equity, it gives you an instant result.

61
00:10:30,000 --> 00:10:36,000
But in my UX video, me and my co-founder ran some experiments to see if we can change this up a little bit.

62
00:10:36,000 --> 00:10:43,000
For example, what if it first clarified before it answered, asked you some clarification questions?

63
00:10:43,000 --> 00:10:51,000
Or another example is, what if it assigned you some homework before it actually gave you a full answer?

64
00:10:52,000 --> 00:10:57,000
Between these options, there are different levels of resistance that the AI is offering.

65
00:10:57,000 --> 00:11:03,000
But what the same author of, when co-pilot becomes autopilot, advocates for is something called productive resistance.

66
00:11:03,000 --> 00:11:05,000
And we haven't yet found what that is.

67
00:11:05,000 --> 00:11:16,000
It's essentially the amount of resistance an AI should give you before you either leave it or go to a simpler AI so that you can do that cognitive off-roading that is so tempting.

68
00:11:17,000 --> 00:11:24,000
But how can we figure out the right amount of productive resistance when open AI and all these companies won't reveal their data sets?

69
00:11:24,000 --> 00:11:27,000
We literally do not know how they train their AI so this day.

70
00:11:27,000 --> 00:11:38,000
When companies themselves don't know how these AI's work and the topic in this example is building an MRI to analyze how the machine they themselves built worked.

71
00:11:38,000 --> 00:11:43,000
This is unprecedented in the history of human technology. We cannot reverse engineer these things.

72
00:11:44,000 --> 00:11:50,000
The solution is likely going to lie between both individuals and the system. It can't be one or the other.

73
00:11:50,000 --> 00:11:55,000
For individuals, we might have to learn what we learned from fitness and nutrition.

74
00:11:55,000 --> 00:12:05,000
For example, maybe we should understand what the elements are good for and what they're not good for, just like with Jim some exercises are better for some things than others.

75
00:12:06,000 --> 00:12:12,000
We should practice using LLMs to assist our thinking rather than replace our thinking.

76
00:12:12,000 --> 00:12:18,000
Again, you wouldn't take a foreclift to the gym, right? The point is to do the reps.

77
00:12:18,000 --> 00:12:29,000
Or maybe you want to make a habit of verifying that the information that LLMs get gives us, just like we look at the back of a food product when they pick it up, the nutrition label.

78
00:12:30,000 --> 00:12:35,000
On a systemic level, we need to look at both governments and education to make changes.

79
00:12:36,000 --> 00:12:43,000
On the schooling level, at least here in North America, I don't think that we treat our kids with the amount of intelligence that they have.

80
00:12:43,000 --> 00:12:49,000
In Finland, kids as young as six years old study, miss and disinformation.

81
00:12:49,000 --> 00:12:55,000
Six years old, we do not talk to these kids about things this complex here. They're clearly capable.

82
00:12:56,000 --> 00:13:02,000
And for governments, we need more regulation, not less, which is exactly what's happening in North America once again.

83
00:13:02,000 --> 00:13:11,000
These AI's cannot be rampant like in the example I gave earlier, just spreading their AI to students, even though they're in the middle of finals when they're most vulnerable.

84
00:13:11,000 --> 00:13:16,000
It has to be a cycle between individual responsibility and responsibility from the system.

85
00:13:17,000 --> 00:13:29,000
Now, as an educator, I love the five W's and H. There are classic for writing essays. The questions, what, why, when, where, who, and how?

86
00:13:29,000 --> 00:13:40,000
And I opened with the question, can AI help us learn? But maybe the question should be, what can AI help us learn? Or how can AI help us learn?

87
00:13:41,000 --> 00:13:48,000
Maybe it should be, why should AI help us learn? Or when and where does AI help with learning?

88
00:13:48,000 --> 00:14:00,000
But the question that scares me the most and that I want to leave you with to reflect on is, who does AI really help when we end up depending on learning with it?

89
00:14:00,000 --> 00:14:02,000
Thank you very much.
