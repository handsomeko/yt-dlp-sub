"""
Configuration management system for yt-dl-sub.

This module provides centralized configuration management using pydantic for validation
and python-dotenv for environment variable loading. Supports all deployment modes
and provides sensible defaults for development.
"""

import os
import re
import logging
from enum import Enum
from pathlib import Path
from typing import Optional, List, Dict, Any, Union, Tuple
from urllib.parse import urlparse

from pydantic import Field, field_validator, model_validator, ValidationError
from pydantic_settings import BaseSettings, SettingsConfigDict
from dotenv import load_dotenv

# Issue #31: Enhanced logging for configuration validation
logger = logging.getLogger(__name__)


# Load environment variables from .env file if it exists
load_dotenv()


class DeploymentMode(str, Enum):
    """Supported deployment modes."""
    LOCAL = "LOCAL"
    MONOLITH = "MONOLITH" 
    DISTRIBUTED = "DISTRIBUTED"


class QueueType(str, Enum):
    """Supported queue types."""
    SQLITE = "sqlite"
    REDIS = "redis"


class LogLevel(str, Enum):
    """Supported log levels."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class StorageBackend(str, Enum):
    """Supported storage backends."""
    LOCAL = "local"
    GDRIVE = "gdrive"
    S3 = "s3"
    GCS = "gcs"


class Settings(BaseSettings):
    """
    Application settings with environment variable support and validation.
    
    All settings can be overridden via environment variables or .env file.
    Environment variable names are automatically generated by uppercasing
    the field name (e.g., deployment_mode -> DEPLOYMENT_MODE).
    """
    
    # Core deployment configuration
    deployment_mode: DeploymentMode = Field(
        default=DeploymentMode.LOCAL,
        description="Deployment mode (LOCAL, MONOLITH, DISTRIBUTED)"
    )
    
    # Storage configuration
    storage_path: Path = Field(
        default=Path.home() / "yt-dl-sub-storage",
        description="Base path for file storage - can be set via STORAGE_PATH env variable"
    )
    storage_version: str = Field(
        default="v2",
        description="Storage structure version: v1 (type-based) or v2 (ID-based with readable names)"
    )
    
    storage_backends: Union[List[StorageBackend], str] = Field(
        default=["local"],
        description="Active storage backends"
    )
    
    # Database configuration
    database_url: str = Field(
        default="sqlite:///data/yt_dl_sub.db",
        description="Database connection URL"
    )
    
    # Queue configuration
    queue_type: QueueType = Field(
        default=QueueType.SQLITE,
        description="Queue backend type"
    )
    
    redis_url: Optional[str] = Field(
        default=None,
        description="Redis URL (required for redis queue type)"
    )
    
    # Worker configuration
    worker_concurrency: int = Field(
        default=3,
        ge=1,
        le=20,
        description="Maximum concurrent workers"
    )
    
    max_retries: int = Field(
        default=3,
        ge=0,
        le=10,
        description="Maximum retry attempts for failed jobs"
    )
    
    retry_delay: int = Field(
        default=300,
        ge=10,
        description="Initial retry delay in seconds"
    )
    
    # YouTube configuration
    youtube_rate_limit: int = Field(
        default=10,
        ge=1,
        le=100,
        description="YouTube requests per minute"
    )
    
    youtube_concurrent_downloads: int = Field(
        default=2,
        ge=1,
        le=10,
        description="Concurrent YouTube downloads"
    )
    
    youtube_burst_size: int = Field(
        default=10,
        ge=1,
        le=20,
        description="Allow burst of requests beyond rate limit"
    )
    
    # Prevention Systems Configuration
    use_enhanced_monitor: bool = Field(
        default=True,
        description="Use EnhancedMonitorWorker with prevention systems"
    )
    
    # Rate Limiting Prevention (Enhanced System) 
    prevention_rate_limit: int = Field(
        default=30,
        ge=5,
        le=100,
        description="Prevention system requests per minute"
    )
    
    prevention_burst_size: int = Field(
        default=10,
        ge=3,
        le=20,
        description="Prevention system burst allowance"
    )
    
    prevention_circuit_breaker_threshold: int = Field(
        default=5,
        ge=3,
        le=10,
        description="Open circuit breaker after N consecutive 429 errors"
    )
    
    prevention_circuit_breaker_timeout: int = Field(
        default=60,
        ge=30,
        le=300,
        description="Circuit breaker recovery timeout in seconds"
    )
    
    prevention_min_request_interval: float = Field(
        default=2.0,
        ge=0.5,
        le=10.0,
        description="Minimum seconds between requests"
    )
    
    prevention_backoff_base: float = Field(
        default=2.0,
        ge=1.5,
        le=3.0,
        description="Exponential backoff base multiplier"
    )
    
    prevention_backoff_max: float = Field(
        default=300.0,
        ge=60.0,
        le=600.0,
        description="Maximum backoff delay in seconds"
    )
    
    # Jittered Rate Limiting Configuration
    prevention_jitter_type: str = Field(
        default="full",
        description="Jitter algorithm type: 'full', 'equal', 'decorrelated', or 'none'"
    )
    
    prevention_base_interval: float = Field(
        default=1.5,
        ge=0.5,
        le=5.0,
        description="Base interval for jittered rate limiting (seconds)"
    )
    
    prevention_jitter_variance: float = Field(
        default=1.0,
        ge=0.1,
        le=2.0,
        description="Jitter variance multiplier (1.0 = full variance)"
    )
    
    # Channel Enumeration Configuration
    default_enumeration_strategy: str = Field(
        default="HYBRID",
        description="Default enumeration strategy (RSS_FEED|YT_DLP_DUMP|YOUTUBE_API|PLAYLIST|HYBRID)"
    )
    
    force_complete_enumeration: bool = Field(
        default=False,
        description="Always get ALL videos from channels"
    )
    
    enumeration_timeout: int = Field(
        default=300,
        ge=60,
        le=1800,
        description="Maximum seconds for channel enumeration"
    )
    
    max_videos_per_channel: Optional[int] = Field(
        default=10000,
        ge=100,
        le=50000,
        description="Maximum videos to enumerate per channel"
    )
    
    cache_duration_hours: int = Field(
        default=24,
        ge=1,
        le=168,
        description="Cache enumeration results for N hours"
    )
    
    incremental_check_interval: int = Field(
        default=3600,
        ge=300,
        le=86400,
        description="Seconds between incremental channel checks"
    )
    
    # Video Discovery Verification
    verify_channel_completeness: bool = Field(
        default=True,
        description="Enable video discovery completeness verification"
    )
    
    deep_check_threshold: int = Field(
        default=100,
        ge=50,
        le=1000,
        description="Enable deep verification for channels with >N videos"
    )
    
    missing_video_confidence: float = Field(
        default=0.85,
        ge=0.5,
        le=0.99,
        description="Confidence threshold for missing video detection"
    )
    
    verification_sample_size: int = Field(
        default=50,
        ge=10,
        le=200,
        description="Sample size for verification checks"
    )
    
    youtube_cooldown_minutes: int = Field(
        default=5,
        ge=1,
        le=60,
        description="Cooldown period after errors in minutes"
    )
    
    youtube_max_failures_before_stop: int = Field(
        default=3,
        ge=1,
        le=10,
        description="Stop after this many consecutive failures"
    )
    
    youtube_backoff_multiplier: float = Field(
        default=2.0,
        ge=1.5,
        le=3.0,
        description="Exponential backoff multiplier for retries"
    )
    
    # Video download configuration
    default_video_quality: str = Field(
        default="1080p",
        description="Default video quality for downloads"
    )
    
    default_video_format: str = Field(
        default="mp4",
        description="Default video format (only used when download_audio_only=False)"
    )
    
    default_audio_format: str = Field(
        default="opus",
        description="Default audio format for transcription"
    )
    
    # Transcription configuration
    whisper_model: str = Field(
        default="base",
        description="Whisper model size (tiny, base, small, medium, large)"
    )
    
    whisper_device: str = Field(
        default="auto",
        description="Device for Whisper (auto, cpu, cuda)"
    )
    
    # Whisper timeout and resource management
    whisper_timeout_base: int = Field(
        default=300,
        ge=60,
        le=3600,
        description="Base timeout for Whisper transcription in seconds"
    )
    
    whisper_timeout_per_minute: float = Field(
        default=2.0,
        ge=1.0,
        le=5.0,
        description="Additional timeout seconds per minute of audio"
    )
    
    whisper_max_duration: int = Field(
        default=7200,
        ge=300,
        le=14400,
        description="Maximum audio duration to process in seconds (2 hours default)"
    )
    
    whisper_chunk_duration: int = Field(
        default=1800,
        ge=300,
        le=3600,
        description="Chunk size for long audio files in seconds (30 minutes default)"
    )
    
    whisper_max_concurrent: int = Field(
        default=2,
        ge=1,
        le=5,
        description="Maximum concurrent Whisper transcription jobs"
    )
    
    whisper_memory_limit_mb: int = Field(
        default=8192,
        ge=2048,
        le=32768,
        description="Memory limit per Whisper job in MB"
    )
    
    whisper_enable_chunking: bool = Field(
        default=True,
        description="Enable chunking for long audio files"
    )
    
    whisper_fallback_models: List[str] = Field(
        default=["base", "tiny"],
        description="Fallback models to try on timeout (smaller/faster models)"
    )
    
    # Two-phase download configuration
    skip_transcription: bool = Field(
        default=False,
        description="Skip Whisper transcription during download (Phase 1 only)"
    )
    
    skip_punctuation: bool = Field(
        default=False,
        description="Skip Chinese punctuation restoration for faster processing"
    )
    
    # API configuration
    api_host: str = Field(
        default="0.0.0.0",
        description="API server host"
    )
    
    api_port: int = Field(
        default=8000,
        ge=1,
        le=65535,
        description="API server port"
    )
    
    api_key_required: bool = Field(
        default=False,
        description="Whether API key authentication is required"
    )
    
    api_keys: Union[List[str], str] = Field(
        default=[],
        description="Valid API keys (comma-separated in env)"
    )
    
    api_rate_limit: int = Field(
        default=60,
        ge=1,
        description="API requests per minute per client"
    )
    
    # Logging configuration
    log_level: LogLevel = Field(
        default=LogLevel.INFO,
        description="Logging level"
    )
    
    # AI Backend Configuration (Phase 1: CLI, Phase 2-3: APIs)
    ai_backend: str = Field(
        default="disabled",
        description="AI backend provider: claude_cli, claude_api, openai_api, gemini_api, disabled"
    )
    ai_model: str = Field(
        default="claude-3-haiku-20240307",
        description="AI model to use for evaluations"
    )
    ai_max_tokens: int = Field(
        default=1000,
        description="Maximum tokens for AI responses"
    )
    
    # Quality Check Settings
    quality_checks_enabled: bool = Field(
        default=False,
        description="Enable AI-based quality checks (costs may apply)"
    )
    quality_check_sample_rate: float = Field(
        default=1.0,
        description="Sample rate for quality checks (1.0 = all, 0.1 = 10%)"
    )
    
    transcript_strictness: str = Field(
        default="standard",
        description="Transcript quality strictness level: lenient, standard, strict"
    )
    
    provider_selection_strategy: str = Field(
        default="weighted_random",
        description="AI provider selection strategy: weighted_random, round_robin, performance_based, cost_optimized"
    )
    
    ab_test_confidence_level: float = Field(
        default=0.95,
        ge=0.80,
        le=0.99,
        description="Statistical confidence level for A/B tests"
    )
    
    ab_test_min_samples: int = Field(
        default=100,
        ge=10,
        description="Minimum samples before making A/B test decisions"
    )
    
    # API Keys (Phase 2+)
    claude_api_key: Optional[str] = Field(
        default=None,
        description="Anthropic Claude API key"
    )
    openai_api_key: Optional[str] = Field(
        default=None,
        description="OpenAI API key"
    )
    gemini_api_key: Optional[str] = Field(
        default=None,
        description="Google Gemini API key"
    )
    
    log_file: Optional[str] = Field(
        default=None,
        description="Log file path (None for stdout only)"
    )
    
    log_max_bytes: int = Field(
        default=10_000_000,  # 10MB
        ge=1_000_000,
        description="Maximum log file size in bytes"
    )
    
    log_backup_count: int = Field(
        default=5,
        ge=1,
        description="Number of log backup files to keep"
    )
    
    # Google Drive configuration
    gdrive_credentials_file: Optional[str] = Field(
        default=None,
        description="Path to Google Drive credentials JSON file"
    )
    
    gdrive_folder_id: Optional[str] = Field(
        default=None,
        description="Google Drive folder ID for uploads"
    )
    
    # Airtable configuration
    airtable_api_key: Optional[str] = Field(
        default=None,
        description="Airtable API key"
    )
    
    airtable_base_id: Optional[str] = Field(
        default=None,
        description="Airtable base ID"
    )
    
    airtable_table_name: str = Field(
        default="Videos",
        description="Airtable table name for video records"
    )
    
    # Content generation configuration (Issue #33: Remove duplicate ai_model)
    content_generation_model: str = Field(
        default="gpt-3.5-turbo",
        description="AI model for content generation"
    )
    
    ai_api_key: Optional[str] = Field(
        default=None,
        description="AI API key (OpenAI, Anthropic, etc.)"
    )
    
    ai_base_url: Optional[str] = Field(
        default=None,
        description="Custom AI API base URL"
    )
    
    content_generators: Union[List[str], str] = Field(
        default=["blog", "social", "summary"],
        description="Active content generators"
    )
    
    # Review checkpoint configuration
    generation_review_required: bool = Field(
        default=True,
        description="Require review before content generation"
    )
    
    auto_approve_generation: bool = Field(
        default=False,
        description="Auto-approve all videos for generation"
    )
    
    # Format selection configuration
    format_selection_required: bool = Field(
        default=True,
        description="Require format selection after video approval"
    )
    
    default_selected_formats: Union[List[str], str] = Field(
        default=["blog", "social"],
        description="Default formats to pre-select"
    )
    
    auto_select_all_formats: bool = Field(
        default=False,
        description="Automatically select all available formats"
    )
    
    channel_default_formats: Union[Dict[str, str], str] = Field(
        default={},
        description="Channel-specific default format selections (channel:formats)"
    )
    
    # AI Summary generation (optional)
    enable_ai_summaries: bool = Field(
        default=False,
        description="Generate AI summaries after transcription"
    )
    
    summary_model: str = Field(
        default="claude-3-haiku-20240307",
        description="AI model for summary generation"
    )
    
    summary_max_tokens: int = Field(
        default=300,
        ge=50,
        le=1000,
        description="Maximum tokens for summary generation"
    )
    
    auto_summarize_channels: Union[List[str], str] = Field(
        default=[],
        description="Channels to always generate summaries for"
    )
    
    # Subtitle Translation Configuration
    subtitle_translation_enabled: bool = Field(
        default=False,
        description="Enable AI translation of non-English subtitles"
    )
    
    subtitle_target_language: str = Field(
        default="en",
        description="Target language for subtitle translation"
    )
    
    subtitle_translation_model: str = Field(
        default="claude-3-haiku-20240307",
        description="AI model for subtitle translation"
    )
    
    subtitle_max_translation_chars: int = Field(
        default=3000,
        ge=500,
        le=10000,
        description="Maximum characters to translate at once (to avoid token limits)"
    )
    
    # Chinese Punctuation Restoration Configuration
    chinese_punctuation_enabled: bool = Field(
        default=False,
        description="Enable AI-powered punctuation restoration for Chinese transcripts"
    )
    
    chinese_punctuation_model: str = Field(
        default="claude-3-haiku-20240307",
        description="AI model for Chinese punctuation restoration"
    )
    
    # Monitoring configuration
    monitor_interval: int = Field(
        default=300,  # 5 minutes
        ge=60,
        description="Channel monitoring interval in seconds"
    )
    
    health_check_interval: int = Field(
        default=30,
        ge=10,
        description="Health check interval in seconds"
    )
    
    # Credential management configuration
    credential_vault_path: str = Field(
        default="credentials/vault.json",
        description="Path to credential vault JSON file"
    )
    
    credential_profile: Optional[str] = Field(
        default=None,
        description="Active credential profile (default, personal, work, client, etc.)"
    )
    
    # Development configuration
    debug: bool = Field(
        default=False,
        description="Enable debug mode"
    )
    
    testing: bool = Field(
        default=False,
        description="Enable testing mode"
    )
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8", 
        case_sensitive=False,
        env_nested_delimiter="__",
    )
    
    @field_validator("storage_version")
    @classmethod
    def validate_storage_version(cls, v: str) -> str:
        """Ensure storage version is valid with enhanced validation."""
        if not v or not isinstance(v, str):
            raise ValueError("storage_version cannot be empty")
        
        v = v.strip().lower()
        if v not in ["v1", "v2"]:
            raise ValueError("storage_version must be 'v1' or 'v2'")
        return v

    @field_validator("storage_path")
    @classmethod
    def validate_storage_path(cls, v: Path) -> Path:
        """Issue #31: Enhanced storage path validation with security checks."""
        if not v:
            raise ValueError("storage_path cannot be empty")
        
        path = Path(v).expanduser().absolute()
        
        # Issue #32: Security validation for path
        path_str = str(path)
        if any(dangerous in path_str for dangerous in ['..', '~/', '$', '`', ';', '|']):
            raise ValueError(f"Storage path contains potentially dangerous characters: {path_str}")
        
        # Check if path is writable
        try:
            path.mkdir(parents=True, exist_ok=True)
            
            # Test write permissions
            test_file = path / '.write_test'
            test_file.write_text('test')
            test_file.unlink()
            
        except (PermissionError, OSError) as e:
            raise ValueError(f"Storage path is not writable: {e}")
        
        return path
    
    @field_validator("database_url")
    @classmethod
    def validate_database_url(cls, v: str) -> str:
        """Issue #31: Enhanced database URL validation with security checks."""
        if not v or not isinstance(v, str):
            raise ValueError("database_url cannot be empty")
        
        # Issue #32: Security validation for database URL
        dangerous_patterns = [';', '--', '/*', '*/', 'union', 'select', 'drop', 'delete']
        v_lower = v.lower()
        for pattern in dangerous_patterns:
            if pattern in v_lower:
                raise ValueError(f"Database URL contains dangerous pattern: {pattern}")
        
        try:
            parsed = urlparse(v)
            allowed_schemes = ["sqlite", "sqlite+aiosqlite", "postgresql", "mysql", "mariadb"]
            if parsed.scheme not in allowed_schemes:
                raise ValueError(f"Unsupported database scheme: {parsed.scheme}. Allowed: {allowed_schemes}")
            
            # For SQLite, validate path security and create directory
            if parsed.scheme in ["sqlite", "sqlite+aiosqlite"]:
                if v.startswith("sqlite:///"):
                    db_path_str = v[10:]  # Remove sqlite:/// prefix
                elif v.startswith("sqlite+aiosqlite:///"):
                    db_path_str = v[20:]  # Remove sqlite+aiosqlite:/// prefix
                else:
                    raise ValueError("SQLite URL must start with sqlite:/// or sqlite+aiosqlite:///")
                
                # Validate path doesn't contain dangerous patterns
                if any(dangerous in db_path_str for dangerous in ['..', '~', '$', '`']):
                    raise ValueError(f"Database path contains dangerous characters: {db_path_str}")
                
                db_path = Path(db_path_str)
                
                # Ensure parent directory exists and is writable
                try:
                    db_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # Test write permissions on parent directory
                    test_file = db_path.parent / '.db_write_test'
                    test_file.write_text('test')
                    test_file.unlink()
                    
                except (PermissionError, OSError) as e:
                    raise ValueError(f"Database directory is not writable: {e}")
                    
            return v
        except Exception as e:
            raise ValueError(f"Invalid database URL: {e}")
    
    @field_validator("whisper_model")
    @classmethod
    def validate_whisper_model(cls, v: str) -> str:
        """Validate Whisper model name."""
        valid_models = ["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"]
        if v not in valid_models:
            raise ValueError(f"Invalid Whisper model. Must be one of: {valid_models}")
        return v
    
    @field_validator("whisper_device")
    @classmethod
    def validate_whisper_device(cls, v: str) -> str:
        """Validate Whisper device."""
        valid_devices = ["auto", "cpu", "cuda"]
        if v not in valid_devices:
            raise ValueError(f"Invalid Whisper device. Must be one of: {valid_devices}")
        return v
    
    @field_validator("default_video_quality")
    @classmethod
    def validate_video_quality(cls, v: str) -> str:
        """Validate video quality."""
        valid_qualities = ["720p", "1080p", "1440p", "2160p", "best", "worst"]
        if v not in valid_qualities:
            raise ValueError(f"Invalid video quality. Must be one of: {valid_qualities}")
        return v
    
    @field_validator("storage_backends", mode='before')
    @classmethod
    def parse_storage_backends(cls, v) -> List[str]:
        """Parse comma-separated storage backends."""
        if isinstance(v, str):
            return [item.strip() for item in v.split(",") if item.strip()]
        elif isinstance(v, list):
            return v
        return ["local"]  # fallback default
    
    @field_validator("api_keys", mode='before')
    @classmethod
    def parse_api_keys(cls, v) -> List[str]:
        """Parse comma-separated API keys."""
        if isinstance(v, str):
            if not v.strip():
                return []
            return [item.strip() for item in v.split(",") if item.strip()]
        elif isinstance(v, list):
            return v
        return []  # fallback default
    
    @field_validator("content_generators", mode='before')
    @classmethod
    def parse_content_generators(cls, v) -> List[str]:
        """Parse comma-separated content generators."""
        if isinstance(v, str):
            return [item.strip() for item in v.split(",") if item.strip()]
        elif isinstance(v, list):
            return v
        return ["blog", "social", "summary"]  # fallback default
    
    @field_validator("auto_summarize_channels", mode='before')
    @classmethod
    def parse_auto_summarize_channels(cls, v) -> List[str]:
        """Parse comma-separated channel names."""
        if isinstance(v, str):
            if not v.strip():
                return []
            return [item.strip() for item in v.split(",") if item.strip()]
        elif isinstance(v, list):
            return v
        return []  # fallback default
    
    @field_validator("default_selected_formats", mode='before')
    @classmethod
    def parse_default_selected_formats(cls, v) -> List[str]:
        """Parse comma-separated default format names."""
        if isinstance(v, str):
            if not v.strip():
                return ["blog", "social"]
            return [item.strip() for item in v.split(",") if item.strip()]
        elif isinstance(v, list):
            return v
        return ["blog", "social"]  # fallback default
    
    @field_validator("channel_default_formats", mode='before')
    @classmethod
    def parse_channel_default_formats(cls, v) -> Dict[str, str]:
        """Parse channel-specific format settings."""
        if isinstance(v, str):
            if not v.strip():
                return {}
            # Parse format: "TED:blog,newsletter;MIT:summary,script"
            result = {}
            for channel_setting in v.split(';'):
                if ':' in channel_setting:
                    channel, formats = channel_setting.split(':', 1)
                    result[channel.strip()] = formats.strip()
            return result
        elif isinstance(v, dict):
            return v
        return {}  # fallback default
    
    # Issue #31, #32, #33: Enhanced validation methods
    
    @field_validator("claude_api_key", "openai_api_key", "gemini_api_key", "ai_api_key", "airtable_api_key")
    @classmethod
    def validate_api_keys(cls, v: Optional[str]) -> Optional[str]:
        """Issue #32: Validate API key format and security."""
        if v is None or v == "":
            return None
        
        # Remove whitespace
        v = v.strip()
        
        # Basic format validation
        if len(v) < 10:
            raise ValueError("API key too short - possible configuration error")
        
        if len(v) > 200:
            raise ValueError("API key too long - possible configuration error")
        
        # Check for dangerous patterns
        if any(char in v for char in [' ', '\n', '\t', '\r']):
            raise ValueError("API key contains invalid whitespace characters")
        
        # Check for placeholder values
        placeholder_patterns = ['your_key', 'api_key', 'replace_me', 'xxx', 'example']
        v_lower = v.lower()
        for pattern in placeholder_patterns:
            if pattern in v_lower:
                logger.warning(f"API key appears to be a placeholder: {pattern}")
                return None  # Treat as not configured
        
        return v
    
    @field_validator("redis_url")
    @classmethod
    def validate_redis_url(cls, v: Optional[str]) -> Optional[str]:
        """Issue #31: Validate Redis URL format."""
        if v is None:
            return None
        
        try:
            parsed = urlparse(v)
            if parsed.scheme not in ['redis', 'rediss']:
                raise ValueError("Redis URL must use redis:// or rediss:// scheme")
            
            if not parsed.hostname:
                raise ValueError("Redis URL must include hostname")
            
            # Validate port if specified
            if parsed.port and (parsed.port < 1 or parsed.port > 65535):
                raise ValueError(f"Invalid Redis port: {parsed.port}")
            
            return v
            
        except Exception as e:
            raise ValueError(f"Invalid Redis URL: {e}")
    
    @field_validator("log_file")
    @classmethod
    def validate_log_file(cls, v: Optional[str]) -> Optional[str]:
        """Issue #31: Validate log file path and permissions."""
        if v is None:
            return None
        
        log_path = Path(v).expanduser().absolute()
        
        # Issue #32: Security validation for log path
        path_str = str(log_path)
        if any(dangerous in path_str for dangerous in ['..', '$', '`', ';', '|']):
            raise ValueError(f"Log file path contains dangerous characters: {path_str}")
        
        # Ensure parent directory is writable
        try:
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Test write permissions
            test_file = log_path.parent / '.log_write_test'
            test_file.write_text('test')
            test_file.unlink()
            
        except (PermissionError, OSError) as e:
            raise ValueError(f"Log file directory is not writable: {e}")
        
        return str(log_path)
    
    @model_validator(mode='after')
    def validate_comprehensive_configuration(self) -> 'Settings':
        """Issue #33: Comprehensive cross-field validation and consistency checks."""
        
        # Validate queue configuration
        if self.queue_type == QueueType.REDIS and not self.redis_url:
            raise ValueError("redis_url is required when queue_type is 'redis'")
        
        # Validate deployment-specific configuration  
        if self.deployment_mode == DeploymentMode.DISTRIBUTED:
            if self.queue_type == QueueType.SQLITE:
                logger.warning(
                    "SQLite queue not recommended for DISTRIBUTED mode. Consider using Redis."
                )
        
        # Issue #33: Validate AI configuration consistency
        if self.quality_checks_enabled:
            if self.ai_backend == "disabled":
                raise ValueError("AI backend must be enabled when quality_checks_enabled is True")
            
            # Validate required AI credentials based on backend
            backend_key_mapping = {
                'claude_api': 'claude_api_key',
                'openai_api': 'openai_api_key', 
                'gemini_api': 'gemini_api_key'
            }
            
            if self.ai_backend in backend_key_mapping:
                required_key = backend_key_mapping[self.ai_backend]
                key_value = getattr(self, required_key, None)
                if not key_value:
                    logger.warning(
                        f"AI backend '{self.ai_backend}' requires {required_key} but none configured"
                    )
        
        # Validate storage backend configuration
        if 'gdrive' in self.storage_backends:
            if not self.gdrive_credentials_file:
                logger.warning("Google Drive storage enabled but no credentials file configured")
        
        # Issue #31: Validate numeric ranges and consistency
        if self.quality_check_sample_rate < 0 or self.quality_check_sample_rate > 1:
            raise ValueError("quality_check_sample_rate must be between 0 and 1")
        
        if self.ab_test_confidence_level < 0.8 or self.ab_test_confidence_level > 0.99:
            raise ValueError("ab_test_confidence_level must be between 0.8 and 0.99")
        
        # Validate worker configuration makes sense
        if self.worker_concurrency > 20:
            logger.warning(f"High worker concurrency ({self.worker_concurrency}) may cause resource issues")
        
        if self.youtube_rate_limit > 60:
            logger.warning(f"High YouTube rate limit ({self.youtube_rate_limit}) may trigger blocks")
        
        return self
    
    @model_validator(mode='before')
    @classmethod
    def validate_cross_field_dependencies(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Issue #31, #32: Enhanced dependency validation with security checks."""
        
        # Issue #32: Handle credential overrides from environment
        override_patterns = {
            'OVERRIDE_GOOGLE_DRIVE_API_KEY': 'gdrive_credentials_file',
            'OVERRIDE_AIRTABLE_API_KEY': 'airtable_api_key',
            'OVERRIDE_CLAUDE_API_KEY': 'claude_api_key',
            'OVERRIDE_OPENAI_API_KEY': 'openai_api_key',
            'OVERRIDE_GEMINI_API_KEY': 'gemini_api_key'
        }
        
        for env_var, setting_key in override_patterns.items():
            override_value = os.getenv(env_var)
            if override_value:
                # Validate override value
                if len(override_value.strip()) < 10:
                    logger.warning(f"Override {env_var} appears invalid (too short)")
                else:
                    values[setting_key] = override_value.strip()
                    logger.info(f"Applied credential override: {env_var}")
        
        # Validate queue configuration
        queue_type = values.get("queue_type")
        redis_url = values.get("redis_url")
        
        if queue_type == "redis" and not redis_url:
            # Issue #34: Fallback to SQLite with warning
            logger.warning("Redis queue requested but no redis_url provided - falling back to SQLite")
            values["queue_type"] = "sqlite"
        
        # Validate deployment-specific configuration  
        deployment_mode = values.get("deployment_mode")
        
        # In distributed mode, ensure proper queue configuration
        if deployment_mode == "DISTRIBUTED":
            if values.get("queue_type") == "sqlite":
                logger.warning(
                    "SQLite queue not recommended for DISTRIBUTED mode - consider Redis for better performance"
                )
        
        return values
    
    @property
    def is_development(self) -> bool:
        """Check if running in development mode."""
        return self.deployment_mode == DeploymentMode.LOCAL or self.debug
    
    @property
    def is_production(self) -> bool:
        """Check if running in production mode."""
        return self.deployment_mode in [DeploymentMode.MONOLITH, DeploymentMode.DISTRIBUTED]
    
    @property
    def database_path(self) -> Optional[Path]:
        """Get database file path for SQLite databases."""
        if self.database_url.startswith("sqlite:///"):
            return Path(self.database_url[10:])
        elif self.database_url.startswith("sqlite+aiosqlite:///"):
            return Path(self.database_url[20:])
        return None
    
    def get_log_config(self) -> Dict[str, Any]:
        """Get logging configuration dict."""
        config = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                    "datefmt": "%Y-%m-%d %H:%M:%S",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(funcName)s - %(message)s",
                    "datefmt": "%Y-%m-%d %H:%M:%S",
                },
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "level": self.log_level.value,
                    "formatter": "default",
                    "stream": "ext://sys.stdout",
                },
            },
            "root": {
                "level": self.log_level.value,
                "handlers": ["console"],
            },
            "loggers": {
                "yt_dl_sub": {
                    "level": self.log_level.value,
                    "handlers": ["console"],
                    "propagate": False,
                },
                "uvicorn": {
                    "level": "INFO",
                    "handlers": ["console"], 
                    "propagate": False,
                },
            },
        }
        
        # Add file handler if log_file is specified
        if self.log_file:
            log_path = Path(self.log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            config["handlers"]["file"] = {
                "class": "logging.handlers.RotatingFileHandler",
                "level": self.log_level.value,
                "formatter": "detailed",
                "filename": str(log_path),
                "maxBytes": self.log_max_bytes,
                "backupCount": self.log_backup_count,
            }
            
            # Add file handler to loggers
            config["root"]["handlers"].append("file")
            config["loggers"]["yt_dl_sub"]["handlers"].append("file")
        
        return config
    
    # Issue #34: Configuration health checking and fallback mechanisms
    
    def validate_service_availability(self) -> Dict[str, Dict[str, Any]]:
        """Check availability and health of configured services."""
        health_status = {}
        
        # Check database connectivity
        if self.database_url:
            health_status['database'] = self._check_database_health()
        
        # Check Redis connectivity if configured
        if self.queue_type == QueueType.REDIS and self.redis_url:
            health_status['redis'] = self._check_redis_health()
        
        # Check storage backends
        health_status['storage'] = self._check_storage_backends_health()
        
        # Check AI services if enabled
        if self.quality_checks_enabled and self.ai_backend != "disabled":
            health_status['ai_service'] = self._check_ai_service_health()
        
        return health_status
    
    def _check_database_health(self) -> Dict[str, Any]:
        """Check database connectivity and health."""
        try:
            if self.database_url.startswith("sqlite"):
                db_path = self.database_path
                if db_path and db_path.exists():
                    # Test read/write access
                    test_query = "SELECT 1"
                    # This would require SQLite connection - simplified check
                    return {
                        'status': 'healthy',
                        'type': 'sqlite',
                        'path': str(db_path),
                        'writable': os.access(db_path.parent, os.W_OK)
                    }
                else:
                    return {
                        'status': 'warning',
                        'type': 'sqlite',
                        'message': 'Database file does not exist yet (will be created)',
                        'path': str(db_path) if db_path else 'unknown'
                    }
            else:
                # For other database types, basic URL validation
                parsed = urlparse(self.database_url)
                return {
                    'status': 'unknown',
                    'type': parsed.scheme,
                    'host': parsed.hostname,
                    'message': 'Cannot test non-SQLite database without connection'
                }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'message': 'Database health check failed'
            }
    
    def _check_redis_health(self) -> Dict[str, Any]:
        """Check Redis connectivity if configured."""
        try:
            parsed = urlparse(self.redis_url)
            return {
                'status': 'unknown',
                'host': parsed.hostname,
                'port': parsed.port or 6379,
                'message': 'Cannot test Redis without connection library'
            }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'message': 'Redis health check failed'
            }
    
    def _check_storage_backends_health(self) -> Dict[str, Any]:
        """Check storage backends health."""
        backend_health = {}
        
        for backend in self.storage_backends:
            if backend == 'local':
                backend_health['local'] = {
                    'status': 'healthy' if self.storage_path.exists() else 'error',
                    'path': str(self.storage_path),
                    'writable': os.access(self.storage_path, os.W_OK) if self.storage_path.exists() else False
                }
            elif backend == 'gdrive':
                creds_exist = bool(self.gdrive_credentials_file and Path(self.gdrive_credentials_file).exists())
                backend_health['gdrive'] = {
                    'status': 'healthy' if creds_exist else 'warning',
                    'credentials_configured': creds_exist,
                    'message': 'Credentials file found' if creds_exist else 'Credentials file not found'
                }
            elif backend == 's3':
                backend_health['s3'] = {
                    'status': 'warning',
                    'message': 'S3 configuration not fully implemented'
                }
            elif backend == 'gcs':
                backend_health['gcs'] = {
                    'status': 'warning', 
                    'message': 'GCS configuration not fully implemented'
                }
        
        return backend_health
    
    def _check_ai_service_health(self) -> Dict[str, Any]:
        """Check AI service configuration health."""
        try:
            backend_status = {
                'backend': self.ai_backend,
                'model': self.ai_model,
                'status': 'unknown'
            }
            
            # Check if required credentials are configured
            if self.ai_backend == 'claude_api':
                backend_status['credentials_configured'] = bool(self.claude_api_key)
                backend_status['status'] = 'healthy' if self.claude_api_key else 'error'
                backend_status['message'] = 'Claude API key configured' if self.claude_api_key else 'Claude API key missing'
            
            elif self.ai_backend == 'openai_api':
                backend_status['credentials_configured'] = bool(self.openai_api_key)
                backend_status['status'] = 'healthy' if self.openai_api_key else 'error'
                backend_status['message'] = 'OpenAI API key configured' if self.openai_api_key else 'OpenAI API key missing'
            
            elif self.ai_backend == 'gemini_api':
                backend_status['credentials_configured'] = bool(self.gemini_api_key)
                backend_status['status'] = 'healthy' if self.gemini_api_key else 'error'
                backend_status['message'] = 'Gemini API key configured' if self.gemini_api_key else 'Gemini API key missing'
            
            elif self.ai_backend == 'claude_cli':
                backend_status['status'] = 'healthy'
                backend_status['message'] = 'CLI backend (no credentials required)'
            
            else:
                backend_status['status'] = 'error'
                backend_status['message'] = f'Unknown AI backend: {self.ai_backend}'
            
            return backend_status
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'message': 'AI service health check failed'
            }
    
    def get_safe_fallback_config(self) -> 'Settings':
        """Get a safe fallback configuration if current config has issues."""
        fallback_values = {
            'deployment_mode': DeploymentMode.LOCAL,
            'storage_path': Path.home() / 'yt-dl-sub-storage-fallback',
            'database_url': 'sqlite:///data/fallback.db',
            'queue_type': QueueType.SQLITE,
            'ai_backend': 'disabled',
            'quality_checks_enabled': False,
            'worker_concurrency': 1,
            'youtube_rate_limit': 5,
            'log_level': LogLevel.INFO,
            'debug': True
        }
        
        try:
            # Create new settings instance with fallback values
            fallback_settings = Settings(**fallback_values)
            logger.warning("Using fallback configuration due to configuration errors")
            return fallback_settings
        except Exception as e:
            logger.error(f"Failed to create fallback configuration: {e}")
            raise ValueError(f"Cannot create safe fallback configuration: {e}")
    
    def validate_and_fix_config(self) -> Tuple['Settings', List[str]]:
        """Validate configuration and attempt to fix issues, returning fixed config and warnings."""
        warnings = []
        fixed_values = {}
        
        try:
            # Check service availability
            health_status = self.validate_service_availability()
            
            # Fix database issues
            if 'database' in health_status and health_status['database']['status'] == 'error':
                warnings.append(f"Database issue detected: {health_status['database'].get('message')}")
                fixed_values['database_url'] = 'sqlite:///data/fixed.db'
                warnings.append("Fixed: Using fallback SQLite database")
            
            # Fix Redis issues
            if 'redis' in health_status and health_status['redis']['status'] == 'error':
                warnings.append(f"Redis issue detected: {health_status['redis'].get('message')}")
                fixed_values['queue_type'] = QueueType.SQLITE
                fixed_values['redis_url'] = None
                warnings.append("Fixed: Switched to SQLite queue from Redis")
            
            # Fix AI service issues
            if 'ai_service' in health_status and health_status['ai_service']['status'] == 'error':
                warnings.append(f"AI service issue detected: {health_status['ai_service'].get('message')}")
                fixed_values['ai_backend'] = 'disabled'
                fixed_values['quality_checks_enabled'] = False
                warnings.append("Fixed: Disabled AI backend due to configuration issues")
            
            # Fix storage issues
            if 'storage' in health_status:
                storage_health = health_status['storage']
                if 'local' in storage_health and storage_health['local']['status'] == 'error':
                    warnings.append("Local storage path issue detected")
                    fixed_values['storage_path'] = Path.home() / 'yt-dl-sub-fixed'
                    warnings.append("Fixed: Using fallback storage path")
            
            # Apply fixes if any
            if fixed_values:
                # Create new Settings instance with fixes
                current_values = self.model_dump()
                current_values.update(fixed_values)
                fixed_settings = Settings(**current_values)
                return fixed_settings, warnings
            else:
                return self, warnings
                
        except Exception as e:
            warnings.append(f"Configuration validation failed: {e}")
            warnings.append("Using safe fallback configuration")
            return self.get_safe_fallback_config(), warnings


# Issue #34: Enhanced settings initialization with error handling
class SafeSettings:
    """Wrapper for Settings that handles initialization errors gracefully."""
    
    def __init__(self):
        self._settings = None
        self._initialization_warnings = []
        self._initialize()
    
    def _initialize(self):
        """Initialize settings with comprehensive error handling."""
        try:
            self._settings = Settings()
            
            # Validate configuration health
            health_status = self._settings.validate_service_availability()
            
            # Check for any critical issues
            critical_issues = []
            for service, status in health_status.items():
                if isinstance(status, dict) and status.get('status') == 'error':
                    critical_issues.append(f"{service}: {status.get('message', 'Unknown error')}")
            
            if critical_issues:
                logger.warning(f"Configuration issues detected: {'; '.join(critical_issues)}")
                
                # Attempt to fix configuration
                fixed_settings, warnings = self._settings.validate_and_fix_config()
                self._settings = fixed_settings
                self._initialization_warnings.extend(warnings)
                
                logger.info(f"Applied {len(warnings)} configuration fixes")
            
        except ValidationError as ve:
            logger.error(f"Settings validation failed: {ve}")
            self._initialization_warnings.append(f"Validation error: {ve}")
            
            try:
                # Try fallback configuration
                self._settings = Settings().get_safe_fallback_config()
                self._initialization_warnings.append("Using fallback configuration due to validation errors")
            except Exception as fallback_error:
                logger.critical(f"Failed to create fallback configuration: {fallback_error}")
                raise RuntimeError(f"Cannot initialize any configuration: {fallback_error}")
        
        except Exception as e:
            logger.error(f"Settings initialization failed: {e}")
            self._initialization_warnings.append(f"Initialization error: {e}")
            
            try:
                # Last resort fallback
                self._settings = Settings().get_safe_fallback_config()
                self._initialization_warnings.append("Using emergency fallback configuration")
            except Exception as emergency_error:
                logger.critical(f"Emergency fallback failed: {emergency_error}")
                raise RuntimeError(f"Complete configuration failure: {emergency_error}")
    
    def get_settings(self) -> Settings:
        """Get the initialized settings instance."""
        if self._settings is None:
            raise RuntimeError("Settings not properly initialized")
        return self._settings
    
    def get_warnings(self) -> List[str]:
        """Get initialization warnings."""
        return self._initialization_warnings.copy()
    
    def reload(self) -> List[str]:
        """Reload settings and return any warnings."""
        self._settings = None
        self._initialization_warnings = []
        self._initialize()
        return self.get_warnings()


# Global settings instance with safe initialization
_safe_settings = SafeSettings()
settings = _safe_settings.get_settings()


def get_settings() -> Settings:
    """Get the global settings instance with safe initialization."""
    return settings


def reload_settings() -> Tuple[Settings, List[str]]:
    """Issue #34: Reload settings with comprehensive error handling and warnings."""
    global settings, _safe_settings
    
    try:
        _safe_settings = SafeSettings()
        settings = _safe_settings.get_settings()
        warnings = _safe_settings.get_warnings()
        
        if warnings:
            for warning in warnings:
                logger.warning(f"Settings reload warning: {warning}")
        
        return settings, warnings
        
    except Exception as e:
        logger.error(f"Failed to reload settings: {e}")
        # Keep existing settings if reload fails
        return settings, [f"Reload failed: {e}"]


def get_configuration_health() -> Dict[str, Any]:
    """Issue #34: Get comprehensive configuration health status."""
    try:
        health_status = settings.validate_service_availability()
        warnings = _safe_settings.get_warnings()
        
        return {
            'overall_status': 'healthy' if not warnings else 'degraded',
            'services': health_status,
            'initialization_warnings': warnings,
            'configuration_valid': True
        }
    except Exception as e:
        return {
            'overall_status': 'error',
            'error': str(e),
            'configuration_valid': False,
            'services': {},
            'initialization_warnings': []
        }


# Convenience functions for common configuration checks
def is_development() -> bool:
    """Check if running in development mode."""
    return settings.is_development


def is_production() -> bool:
    """Check if running in production mode."""
    return settings.is_production


def get_storage_path() -> Path:
    """Get the configured storage path."""
    return settings.storage_path


def get_database_url() -> str:
    """Get the configured database URL."""
    return settings.database_url